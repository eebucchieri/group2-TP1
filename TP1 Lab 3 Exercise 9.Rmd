---
title: "Chapter6L2L3"
author: "Team2"
date: "1/28/2020"
output:
  html_document: default
  pdf_document: default
---


```{r include = FALSE}
library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))

set.seed(1) 
train=sample(c(TRUE ,FALSE), nrow(Hitters ),rep=TRUE)
test=(!train)
```


```{r include = FALSE}
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
```

```{r include = FALSE}
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients",x=x[train,],y=y[train])[1:20,]
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
```


```{r include = FALSE}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

# Chapter 6 Lab 3: PCR and PLS Regression

# Principal Components Regression
## Step 1: Run the regression
```{r}
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters,scale=TRUE,validation="CV") 
# scale = TRUE: standadizes the predictors 
# validation="CV": computes the ten-fold cross-validation error for each possible value of M
# M: the number of principal components used
summary(pcr.fit)
```

## Step 2: Plot the cross-validation scores 
```{r}
validationplot(pcr.fit,val.type="MSEP") 
# notice that smallest cross-validation error occurs when M = 16, and it's roughly the same to M = 1
# suggests that a model that uses just a small number of components might suffice
```

## Step 3: perform PCR on the training data and evaluate its test set performance
```{r}
set.seed(1)
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation="CV")
validationplot(pcr.fit,val.type="MSEP")
# lowest cross-validation error occurs when M =7
```

## Step 4: compute the test MSE with selected M
```{r}
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
# test MSE when M = 7 is 140751.3 
mean((pcr.pred-y.test)^2) # Competitive result...however more difficult to interpert 
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
```

# Partial Least Squares
## Step 1: run regression 
```{r}
set.seed(1)
pls.fit=plsr(Salary~., data=Hitters,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
# min cross-validation error occurs when only M = 2 partial least squares directions are used 
```

## Step 2: now evaluate the corresponding test set MSE
```{r}
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2) #Slightly higher test MSE when compared to Ridge, Lasso, and PCR 
pls.fit=plsr(Salary~., data=Hitters,scale=TRUE,ncomp=2) 
#Performs PLS using the full data set, using M = 2, the no. of components identified by corss validation
summary(pls.fit) #The percentage of varaince is similar to that of PCR. 
#PCR attempts to maximize the amount of variance explained in the predictors
#PLS searches for directions that explain the varaince in both the prdictos AND the response 
```

# Applied Exercise 9
In this exercise, we will predict the number of applications received using the other variables in the College data set.

(a) Split the data set into a training set and a test set.
```{r}
library(ISLR)
set.seed(111)
sum(is.na(College))
```

```{r}
# split data into train and test set
train.size = dim(College)[1] / 2
train = sample(1:dim(College)[1], train.size)
test = -train
College.train = College[train, ]
College.test = College[test, ]
```

(b) Fit a linear model using least squares on the training set, and report the test error obtained. 
```{r}
lm.fit = lm(Apps~., data=College.train)
lm.pred = predict(lm.fit, College.test)
mean((College.test[, "Apps"] - lm.pred)^2)
```

(c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.
```{r}
require(glmnet)
# Creating matrix for test set and train set
train.mat = model.matrix(Apps~., data=College.train)
test.mat = model.matrix(Apps~., data=College.test)
grid = 10 ^ seq(4, -2, length=100)


# fitting a cv model, alpha = 0 means ridge regression 
mod.ridge = cv.glmnet(train.mat, College.train[, "Apps"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = mod.ridge$lambda.min
lambda.best

# calculating the test error 
ridge.pred = predict(mod.ridge, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - ridge.pred)^2)
# the test error is slightly higher than OLS
```

(d) Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coeﬃcient estimates. 
```{r}
mod.lasso = cv.glmnet(train.mat, College.train[, "Apps"], alpha=1, lambda=grid, thresh=1e-12)
lambda.best = mod.lasso$lambda.min
lambda.best

lasso.pred = predict(mod.lasso, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - lasso.pred)^2)
# the test error is slightly higher than OLS but smaller than Ridge
```

```{r}
# non zero coefficients
lasso.coef=predict(mod.lasso,type="coefficients",s=lambda.best)
length(lasso.coef[lasso.coef!=0]) # 15 non-zero coefficients
lasso.coef # list of non-zero coefficients
```

(e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation. 
```{r}
library(pls)
pcr.fit = pcr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
```

```{r}
pcr.pred = predict(pcr.fit, College.test, ncomp=10)
mean((College.test[, "Apps"] - pcr.pred)^2)
# the highest test error so far
```

(f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation. 
```{r}
pls.fit = plsr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pls.fit, val.type="MSEP")
```

```{r}
pls.pred = predict(pls.fit, College.test, ncomp=10)
mean((College.test[, "Apps"] - pls.pred)^2)
# test error is smaller than OLS
```

(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much diﬀerence among the test errors resulting from these ﬁve approaches?
```{r}
# calculating the test R square to compare models
test.avg = mean(College.test[, "Apps"])
lm.test.r2 = 1 - mean((College.test[, "Apps"] - lm.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
ridge.test.r2 = 1 - mean((College.test[, "Apps"] - ridge.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
lasso.test.r2 = 1 - mean((College.test[, "Apps"] - lasso.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
pcr.test.r2 = 1 - mean((College.test[, "Apps"] - pcr.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
pls.test.r2 = 1 - mean((College.test[, "Apps"] - pls.pred)^2) /mean((College.test[, "Apps"] - test.avg)^2)
barplot(c(lm.test.r2, ridge.test.r2, lasso.test.r2, pcr.test.r2, pls.test.r2), col="red", names.arg=c("OLS", "Ridge", "Lasso", "PCR", "PLS"), main="Test R-squared")
```
The plot shows that test R2 for all models except PCR are around 0.9, with PLS having slightly higher test R2 than others. PCR has a smaller test R2 of less than 0.8. All models except PCR predict college applications with high accuracy.